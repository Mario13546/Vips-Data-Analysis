{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c99b1c6e",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "711c4352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "901d7836",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = \"08-27-2025\"\n",
    "data_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c11cdd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_str       = \"../graphs/\" + date + \"/\"\n",
    "analysis_str    = \"../analysis/\" + date + \"/\"\n",
    "directory_str   = \"../data/\" + date + \"/\"\n",
    "\n",
    "for dir in os.listdir(directory_str):\n",
    "    data_dict[dir] = []\n",
    "\n",
    "    tmp_graph_str     = graph_str + dir + \"/\"\n",
    "    tmp_directory_str = directory_str + dir + \"/\"\n",
    "\n",
    "    if os.path.isdir(analysis_str) == False:\n",
    "        os.makedirs(analysis_str)\n",
    "\n",
    "    if os.path.isdir(tmp_graph_str) == False:\n",
    "        os.makedirs(tmp_graph_str)\n",
    "\n",
    "    for file in os.listdir(tmp_directory_str):\n",
    "        data_dict[dir].append(pd.read_csv(tmp_directory_str + file, delimiter=\";\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71dbd1b",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcaaa355",
   "metadata": {},
   "outputs": [],
   "source": [
    "COL_NAMES = {\n",
    "    \"X_COL_1\": \"Flow EZ #2 (12807)\",\n",
    "    \"Y_COL_1\": \"Flow Unit #1 [Flow EZ #2 (12807)]\",\n",
    "}\n",
    "num_graphs = math.floor(COL_NAMES.keys().__len__() / 2)\n",
    "i_range = list(range(1, num_graphs + 1))\n",
    "\n",
    "filtered_dict = {}\n",
    "\n",
    "COL_LIST = list(COL_NAMES.values())\n",
    "COL_LIST.append(\"Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df9b8fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data_dict.keys():\n",
    "    filtered_dict[key] = []\n",
    "\n",
    "    for df in data_dict[key]:\n",
    "        filtered_dict[key].append(df[COL_LIST])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72510a74",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12a90bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "from typing import Tuple\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ab5d869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_logarithmic_fit(df: pd.DataFrame) -> list[str]:\n",
    "    \"\"\"\n",
    "    Use scipy to find the logarithmic fit and fitting parameters\n",
    "    \"\"\"\n",
    "    log_eqs = []\n",
    "\n",
    "    for i in i_range:\n",
    "        x_col = COL_NAMES[f\"X_COL_{i}\"]\n",
    "        y_col = COL_NAMES[f\"Y_COL_{i}\"]\n",
    "        log_fit = f\"log_fit_{i}\"\n",
    "\n",
    "        log_x = np.log(df[x_col])\n",
    "        y = df[y_col]\n",
    "\n",
    "        a_log, b_log, r_log, _, _ = stats.linregress(log_x, y)\n",
    "\n",
    "        df[log_fit] = a_log * log_x + b_log\n",
    "\n",
    "        y_true     = df[y_col]\n",
    "        y_pred_log = df[log_fit]\n",
    "        ss_res_log = np.sum((y_true - y_pred_log) ** 2)\n",
    "        ss_tot_log = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "        r2_log = 1 - (ss_res_log / ss_tot_log)\n",
    "\n",
    "        if not np.isnan(a_log):\n",
    "            log_eqs.append((\n",
    "                f\"Log: y = {a_log:.3f} ln(x) + {b_log:.3f}\\n\"\n",
    "                f\"R² = {r2_log:.3f}\"\n",
    "            ))\n",
    "        else:\n",
    "            df['log_fit'] = np.nan\n",
    "            log_eqs.append(\"Logarithmic fit failed\")\n",
    "\n",
    "    return log_eqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "395a33e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_logistic_fit(df: pd.DataFrame) -> list[str]:\n",
    "    \"\"\"\n",
    "    Use scipy to find the logistic fit and fitting parameters\n",
    "    \"\"\"\n",
    "    logistic_eqs = []\n",
    "\n",
    "    # Logistic function definition\n",
    "    def logistic(x, L, k, x0):\n",
    "        return L / (1 + np.exp(-k * (x - x0)))\n",
    "\n",
    "    for i in i_range:\n",
    "        x_col = COL_NAMES[f\"X_COL_{i}\"]\n",
    "        y_col = COL_NAMES[f\"Y_COL_{i}\"]\n",
    "        logistic_fit = f\"logistic_fit_{i}\"\n",
    "\n",
    "        # Initial parameter guess: [max y, slope, midpoint]\n",
    "        initial_guess = [df[y_col].max(), 1, df[x_col].median()]\n",
    "\n",
    "        # Fit the logistic model\n",
    "        try:\n",
    "            params, _ = curve_fit(logistic, df[x_col], df[y_col], p0=initial_guess, maxfev=10000)\n",
    "            L, k, x0_log = params\n",
    "            df[logistic_fit] = logistic(df[x_col], L, k, x0_log)\n",
    "\n",
    "            # Calculate R²\n",
    "            y_true = df[y_col]\n",
    "            y_pred = df[logistic_fit]\n",
    "            ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "            ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "            r2_logistic = 1 - (ss_res / ss_tot)\n",
    "\n",
    "            logistic_eqs.append((\n",
    "                f\"Logistic: y = {L:.2f} / (1 + e^(-{k:.2f}(x - {x0_log:.2f})))\\n\"\n",
    "                f\"R² = {r2_logistic:.3f}\"\n",
    "            ))\n",
    "        except RuntimeError:\n",
    "            df[logistic_fit] = np.nan\n",
    "            logistic_eqs.append(\"Logistic fit failed\")\n",
    "\n",
    "    return logistic_eqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b012d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lin_fit(df: pd.DataFrame) -> list[str]:\n",
    "    \"\"\"\n",
    "    Use scipy to find the linear fit and fitting parameters\n",
    "    \"\"\"\n",
    "    lin_eqs = []\n",
    "\n",
    "    for i in i_range:\n",
    "        x_col = COL_NAMES[f\"X_COL_{i}\"]\n",
    "        y_col = COL_NAMES[f\"Y_COL_{i}\"]\n",
    "        lin_fit = f\"lin_fit_{i}\"\n",
    "\n",
    "        a_lin, b_lin, r_lin, _, _ = stats.linregress(df[x_col], df[y_col])\n",
    "\n",
    "        df[lin_fit] = a_lin * df[x_col] + b_lin\n",
    "\n",
    "        y_true     = df[y_col]\n",
    "        y_pred_lin = df[lin_fit]\n",
    "        ss_res_lin = np.sum((y_true - y_pred_lin) ** 2)\n",
    "        ss_tot_lin = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "        r2_lin = 1 - (ss_res_lin / ss_tot_lin)\n",
    "\n",
    "        lin_eqs.append((\n",
    "            f\"Linear: y = {a_lin:.3f}x + {b_lin:.3f}\\n\"\n",
    "            f\"R² = {r2_lin:.3f}\"\n",
    "        ))\n",
    "\n",
    "    return lin_eqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1378ccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def melt_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Melt data for legend support\n",
    "    \"\"\"\n",
    "    # name_vars = [f\"Logarithmic Fit {i}\" for i in i_range] + [f\"Linear Fit {i}\" for i in i_range] + [f\"Logistic Fit {i}\" for i in i_range]\n",
    "    # value_vars = [f\"log_fit_{i}\" for i in i_range] + [f\"lin_fit_{i}\" for i in i_range] + [f\"logistic_fit_{i}\" for i in i_range]\n",
    "    name_vars = [f\"Linear Fit {i}\" for i in i_range] + [f\"Logistic Fit {i}\" for i in i_range]\n",
    "    value_vars = [f\"lin_fit_{i}\" for i in i_range] + [f\"logistic_fit_{i}\" for i in i_range]\n",
    "\n",
    "    df_melt = pd.melt(\n",
    "        df,\n",
    "        id_vars=COL_NAMES.values(),\n",
    "        value_vars=value_vars,\n",
    "        var_name='Fit Type',\n",
    "        value_name='Fit Value'\n",
    "    )\n",
    "\n",
    "    # Rename for legend\n",
    "    fit_labels = {value_vars[i]: name_vars[i] for i in range(len(name_vars))}\n",
    "    df_melt['Fit Type'] = df_melt['Fit Type'].map(fit_labels)\n",
    "\n",
    "    return df_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a22a8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square_test(df_melt: pd.DataFrame, chip: str, alpha: float = 0.05):\n",
    "    \"\"\"\n",
    "    Perform Chi-Square test\n",
    "    \"\"\"\n",
    "    def calculate_bins(data: list) -> int:\n",
    "        \"\"\"\n",
    "        Calculate the number of histogram bins using the Freedman-Diaconis rule.\n",
    "        \"\"\"\n",
    "        # Pre-process\n",
    "        data = np.asarray(data)\n",
    "        data = data[~np.isnan(data)]\n",
    "\n",
    "        q75, q25 = np.percentile(data, [75 ,25])\n",
    "        iqr = q75 - q25\n",
    "        n = len(data)\n",
    "\n",
    "        # Fallback: just one bin\n",
    "        if iqr == 0 or n <= 1:\n",
    "            return 1\n",
    "\n",
    "        bin_width = 2 * iqr / (n ** (1 / 3))\n",
    "\n",
    "        # Avoid division by zero\n",
    "        if bin_width == 0:\n",
    "            return 1\n",
    "\n",
    "        data_range = data.max() - data.min()\n",
    "        bins = int(np.ceil(data_range / bin_width))\n",
    "\n",
    "        # Always at least one bin\n",
    "        return max(bins, 1)\n",
    "\n",
    "    for i in i_range:\n",
    "        x_col = COL_NAMES[f\"X_COL_{i}\"]\n",
    "        y_col = COL_NAMES[f\"Y_COL_{i}\"]\n",
    "\n",
    "        # Replace with your DataFrame and column name\n",
    "        df = df_melt.copy()\n",
    "        x_data = df[x_col]\n",
    "        y_data = df[y_col]\n",
    "\n",
    "        # Calculate the number of bins using Freedman-Diaconis rule\n",
    "        x_bins = calculate_bins(x_data)\n",
    "        y_bins = calculate_bins(y_data)\n",
    "\n",
    "        # Bin the x and y data\n",
    "        df['x_binned'] = pd.cut(x_data, bins=x_bins)\n",
    "        df['y_binned'] = pd.cut(y_data, bins=y_bins)\n",
    "\n",
    "        # Create contingency table (cross-tab of binned values)\n",
    "        contingency_table = pd.crosstab(df['x_binned'], df['y_binned'])\n",
    "\n",
    "        # Perform chi-squared test\n",
    "        chi2, p, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "\n",
    "        # Write data to file\n",
    "        with open(analysis_str + f\"{chip}_chi_square_results_{i}.txt\", \"w\") as f:\n",
    "            # Log results to file\n",
    "            f.write(f\"Chi-squared statistic: {chi2}\\n\")\n",
    "            f.write(f\"Degrees of freedom: {dof}\\n\")\n",
    "            f.write(f\"Optimal bin count: x = {x_bins}, y = {y_bins}\\n\")\n",
    "            f.write(f\"Expected frequencies:\\n{expected}\\n\")\n",
    "            f.write(f\"P-value: {p}\\n\")\n",
    "\n",
    "            # Interpret the result\n",
    "            if p < alpha:\n",
    "                f.write(\"Result: Reject the null hypothesis — association exists between binned x and y.\")\n",
    "            else:\n",
    "                f.write(\"Result: Fail to reject the null — no significant association.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dc0a4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anova_test(y_vals: list, alpha: float = 0.05) -> None:\n",
    "    \"\"\"\n",
    "    Perform one-way ANOVA test\n",
    "    \"\"\"\n",
    "    # Number of groups and total number of observations\n",
    "    k = len(y_vals)\n",
    "    n = 0\n",
    "\n",
    "    for group in y_vals:\n",
    "        n += len(group)\n",
    "\n",
    "    # Perform one-way ANOVA\n",
    "    f_statistic, p_value = stats.f_oneway(*y_vals)\n",
    "\n",
    "    # Degrees of freedom\n",
    "    df_between = k - 1\n",
    "    df_within = n - k\n",
    "\n",
    "    # Calculate critical F value\n",
    "    f_critical = stats.f.ppf(1 - alpha, df_between, df_within)\n",
    "\n",
    "    # Write data to file\n",
    "    with open(analysis_str + \"anova_results.txt\", \"w\") as f:\n",
    "        # Log results to file\n",
    "        f.write(f\"F-statistic: {f_statistic:.5f}\\n\")\n",
    "        f.write(f\"p-value: {p_value:.5f}\\n\")\n",
    "        f.write(f\"F-critical (alpha = {alpha}): {f_critical:.5f}\\n\")\n",
    "\n",
    "        # Interpret the result\n",
    "        if f_statistic > f_critical:\n",
    "            f.write(f\"Reject the null hypothesis: At least one group mean is different.\")\n",
    "        else:\n",
    "            f.write(f\"Fail to reject the null hypothesis: No significant difference between group means.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d7a15a",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "390072c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import (\n",
    "    ggplot,\n",
    "    aes,\n",
    "    geom_point,\n",
    "    geom_line,\n",
    "    ggtitle,\n",
    "    labs,\n",
    "    annotate,\n",
    "    xlim,\n",
    "    ylim,\n",
    "    scale_color_manual,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81c8db00",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_LABEL = \"Pressure In (mb)\"\n",
    "Y_LABEL = \"Flow Rate Out (µL/min)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32cca44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make variables\n",
    "data_list = []\n",
    "plot_layers = []\n",
    "colors = [\n",
    "    \"black\",\n",
    "    \"purple\",\n",
    "    \"orange\"\n",
    "]\n",
    "\n",
    "x_min = 0\n",
    "y_min = 0\n",
    "x_max = 100\n",
    "y_max = 1000\n",
    "annotation_step = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0285c3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_404849/3353449167.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/tmp/ipykernel_404849/217409766.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/srv/vips_data_analysis/.venv/lib/python3.10/site-packages/plotnine/ggplot.py:615: PlotnineWarning: Saving 16 x 9 in image.\n",
      "/srv/vips_data_analysis/.venv/lib/python3.10/site-packages/plotnine/ggplot.py:616: PlotnineWarning: Filename: ../graphs/08-27-2025/chip_1/graph_0-1.png\n",
      "/srv/vips_data_analysis/.venv/lib/python3.10/site-packages/plotnine/layer.py:364: PlotnineWarning: geom_point : Removed 1152 rows containing missing values.\n",
      "/srv/vips_data_analysis/.venv/lib/python3.10/site-packages/plotnine/geoms/geom_path.py:100: PlotnineWarning: geom_path: Removed 750 rows containing missing values.\n",
      "/tmp/ipykernel_404849/3353449167.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/tmp/ipykernel_404849/217409766.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/srv/vips_data_analysis/.venv/lib/python3.10/site-packages/plotnine/ggplot.py:615: PlotnineWarning: Saving 16 x 9 in image.\n",
      "/srv/vips_data_analysis/.venv/lib/python3.10/site-packages/plotnine/ggplot.py:616: PlotnineWarning: Filename: ../graphs/08-27-2025/chip_1/graph_1-1.png\n",
      "/srv/vips_data_analysis/.venv/lib/python3.10/site-packages/plotnine/layer.py:364: PlotnineWarning: geom_point : Removed 88 rows containing missing values.\n",
      "/srv/vips_data_analysis/.venv/lib/python3.10/site-packages/plotnine/geoms/geom_path.py:100: PlotnineWarning: geom_path: Removed 66 rows containing missing values.\n"
     ]
    }
   ],
   "source": [
    "# Graph all the data\n",
    "for key in filtered_dict.keys():\n",
    "    df = pd.DataFrame()\n",
    "    chip_list = filtered_dict[key]\n",
    "\n",
    "    for i, df in enumerate(chip_list):\n",
    "        # Find applicable fits\n",
    "        # log_eqs      = find_logarithmic_fit(df)\n",
    "        lin_eqs      = find_lin_fit(df)\n",
    "        logistic_eqs = find_logistic_fit(df)\n",
    "\n",
    "        # Melt the data\n",
    "        df_melt = melt_data(df)\n",
    "\n",
    "        for j in i_range:\n",
    "            x_col = COL_NAMES[f\"X_COL_{j}\"]\n",
    "            y_col = COL_NAMES[f\"Y_COL_{j}\"]\n",
    "\n",
    "            x_min = 0\n",
    "            y_min = df_melt[y_col].min()\n",
    "            x_max = df_melt[x_col].max()\n",
    "            y_max = df_melt[y_col].max()\n",
    "\n",
    "            annotation_step = ((y_max - y_min) / 4) / 3\n",
    "\n",
    "            # Make the plot\n",
    "            plot =  (\n",
    "                ggplot(df_melt, aes(x_col, y_col)) +\n",
    "                geom_point() +\n",
    "                ggtitle(f\"{Y_LABEL} vs {X_LABEL}\") +\n",
    "                geom_line(\n",
    "                    aes(\n",
    "                        y='Fit Value',\n",
    "                        color='Fit Type'\n",
    "                    ),\n",
    "                    data=df_melt[df_melt[\"Fit Type\"].str.endswith(f\"{j}\")]\n",
    "                ) +\n",
    "                scale_color_manual(\n",
    "                    values = {\n",
    "                        # f'Logarithmic Fit {j}': 'blue',\n",
    "                        f'Linear Fit {j}': 'red',\n",
    "                        f'Logistic Fit {j}': 'green'\n",
    "                    },\n",
    "                    labels={\n",
    "                        # f'Logarithmic Fit {j}': 'Logarithmic Fit',\n",
    "                        f'Linear Fit {j}': 'Linear Fit',\n",
    "                        f'Logistic Fit {j}': 'Logistic Fit'\n",
    "                    }\n",
    "                ) +\n",
    "                labs(\n",
    "                    x=X_LABEL,\n",
    "                    y=Y_LABEL,\n",
    "                    color=\"Model Fit\"\n",
    "                ) +\n",
    "                xlim(0, x_max) +\n",
    "                ylim(0, y_max) +\n",
    "                annotate(\n",
    "                    \"text\",\n",
    "                    x=0,\n",
    "                    y=y_max - annotation_step * 1,\n",
    "                    label=logistic_eqs[j - 1],\n",
    "                    ha='left',\n",
    "                    va='top',\n",
    "                    size=8,\n",
    "                    color='green'\n",
    "                ) +\n",
    "                annotate(\n",
    "                    \"text\",\n",
    "                    x=0,\n",
    "                    y=y_max - annotation_step * 0,\n",
    "                    label=lin_eqs[j - 1],\n",
    "                    ha='left',\n",
    "                    va='top',\n",
    "                    size=8,\n",
    "                    color='red'\n",
    "                )\n",
    "                # ) +\n",
    "                # annotate(\n",
    "                #     \"text\",\n",
    "                #     x=0,\n",
    "                #     y=y_max - annotation_step * 0, # If included, this should be the zero to keep with legend ordering\n",
    "                #     label=log_eqs[j - 1],\n",
    "                #     ha='left',\n",
    "                #     va='top',\n",
    "                #     size=8,\n",
    "                #     color='blue'\n",
    "                # )\n",
    "            )\n",
    "\n",
    "            # Save the plot\n",
    "            plot.save(graph_str + f\"{key}/graph_{i}-{j}.png\", width=16, height=9, dpi=600)\n",
    "\n",
    "            # Add data to the analysis list\n",
    "            data_list.append(df_melt[y_col].tolist())\n",
    "\n",
    "        # Perform Chi-Square test\n",
    "        chi_square_test(df_melt, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a352198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform anova analysis\n",
    "anova_test(data_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
