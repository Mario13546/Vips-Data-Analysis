{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c99b1c6e",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711c4352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901d7836",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = \"04-17-2025\"\n",
    "data_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11cdd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_str       = \"../graphs/\" + date + \"/\"\n",
    "analysis_str    = \"../analysis/\" + date + \"/\"\n",
    "directory_str   = \"../data/\" + date + \"/\"\n",
    "\n",
    "for dir in os.listdir(directory_str):\n",
    "    data_dict[dir] = []\n",
    "\n",
    "    tmp_graph_str     = graph_str + dir + \"/\"\n",
    "    tmp_directory_str = directory_str + dir + \"/\"\n",
    "\n",
    "    if os.path.isdir(analysis_str) == False:\n",
    "        os.makedirs(analysis_str)\n",
    "\n",
    "    if os.path.isdir(tmp_graph_str) == False:\n",
    "        os.makedirs(tmp_graph_str)\n",
    "\n",
    "    for file in os.listdir(tmp_directory_str):\n",
    "        data_dict[dir].append(pd.read_csv(tmp_directory_str + file, delimiter=\";\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71dbd1b",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaaa355",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_COL = \"Flow EZ #2 (12806)\"\n",
    "Y_COL = \"Flow Unit #1 [Flow EZ #1 (11411)]\"\n",
    "\n",
    "filtered_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9b8fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data_dict.keys():\n",
    "    filtered_dict[key] = []\n",
    "\n",
    "    for df in data_dict[key]:\n",
    "        filtered_dict[key].append(df[[X_COL, Y_COL]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72510a74",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a90bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "from typing import Tuple\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab5d869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_log_fit(df: pd.DataFrame) -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Use scipy to find the logarithmic fit and fitting parameters\n",
    "    \"\"\"\n",
    "\n",
    "    log_x = np.log(df[X_COL])\n",
    "    y = df[Y_COL]\n",
    "\n",
    "    a_log, b_log, r_log, _, _ = stats.linregress(log_x, y)\n",
    "\n",
    "    df['log_fit'] = a_log * log_x + b_log\n",
    "\n",
    "    y_true     = df[Y_COL]\n",
    "    y_pred_log = df['log_fit']\n",
    "    ss_res_log = np.sum((y_true - y_pred_log) ** 2)\n",
    "    ss_tot_log = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    r2_log = 1 - (ss_res_log / ss_tot_log)\n",
    "\n",
    "    if not np.isnan(a_log):\n",
    "        log_eq = (\n",
    "            f\"Log: y = {a_log:.3f} ln(x) + {b_log:.3f}\\n\"\n",
    "            f\"R² = {r2_log:.3f}\"\n",
    "        )\n",
    "    else:\n",
    "        df['log_fit'] = np.nan\n",
    "        log_eq = \"Logarithmic fit failed\"\n",
    "\n",
    "    return log_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395a33e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_logistic_fit(df: pd.DataFrame) -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Use scipy to find the logistic fit and fitting parameters\n",
    "    \"\"\"\n",
    "\n",
    "    # Logistic function definition\n",
    "    def logistic(x, L, k, x0):\n",
    "        return L / (1 + np.exp(-k * (x - x0)))\n",
    "    \n",
    "    # Initial parameter guess: [max y, slope, midpoint]\n",
    "    initial_guess = [df[Y_COL].max(), 1, df[X_COL].median()]\n",
    "\n",
    "    # Fit the logistic model\n",
    "    try:\n",
    "        params, _ = curve_fit(logistic, df[X_COL], df[Y_COL], p0=initial_guess, maxfev=10000)\n",
    "        L, k, x0_log = params\n",
    "        df['logistic_fit'] = logistic(df[X_COL], L, k, x0_log)\n",
    "        logistic_eq = f\"Logistic: y = {L:.2f} / (1 + e^(-{k:.2f}(x - {x0_log:.2f})))\"\n",
    "\n",
    "        # Calculate R²\n",
    "        y_true = df[Y_COL]\n",
    "        y_pred = df['logistic_fit']\n",
    "        ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "        r2_logistic = 1 - (ss_res / ss_tot)\n",
    "\n",
    "        logistic_eq = (\n",
    "            f\"Logistic: y = {L:.2f} / (1 + e^(-{k:.2f}(x - {x0_log:.2f})))\\n\"\n",
    "            f\"R² = {r2_logistic:.3f}\"\n",
    "        )\n",
    "    except RuntimeError:\n",
    "        df['logistic_fit'] = np.nan\n",
    "        logistic_eq = \"Logistic fit failed\"\n",
    "\n",
    "    return logistic_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b012d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lin_fit(df: pd.DataFrame) -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Use scipy to find the linear fit and fitting parameters\n",
    "    \"\"\"\n",
    "\n",
    "    a_lin, b_lin, r_lin, _, _ = stats.linregress(df[X_COL], df[Y_COL])\n",
    "\n",
    "    df['lin_fit'] = a_lin * df[X_COL] + b_lin\n",
    "\n",
    "    y_true     = df[Y_COL]\n",
    "    y_pred_lin = df['lin_fit']\n",
    "    ss_res_lin = np.sum((y_true - y_pred_lin) ** 2)\n",
    "    ss_tot_lin = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    r2_lin = 1 - (ss_res_lin / ss_tot_lin)\n",
    "\n",
    "    lin_eq = (\n",
    "        f\"Log: y = {a_lin:.3f} ln(x) + {b_lin:.3f}\\n\"\n",
    "        f\"R² = {r2_lin:.3f}\"\n",
    "    )\n",
    "\n",
    "    return lin_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1378ccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def melt_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Melt data for legend support\n",
    "    \"\"\"\n",
    "\n",
    "    df_melt = pd.melt(\n",
    "        df,\n",
    "        id_vars=[X_COL, Y_COL],\n",
    "        value_vars=['log_fit', 'lin_fit', 'logistic_fit'],\n",
    "        var_name='Fit Type',\n",
    "        value_name='Fit Value'\n",
    "    )\n",
    "\n",
    "    # Rename for legend\n",
    "    fit_labels = {'log_fit': 'Logarithmic Fit', 'lin_fit': 'Linear Fit', 'logistic_fit': 'Logistic Fit'}\n",
    "    df_melt['Fit Type'] = df_melt['Fit Type'].map(fit_labels)\n",
    "\n",
    "    return df_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a22a8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square_test(df_melt: pd.DataFrame, chip: str, alpha: float = 0.05):\n",
    "    \"\"\"\n",
    "    Perform Chi-Square test\n",
    "    \"\"\"\n",
    "    def calculate_bins(data: list) -> int:\n",
    "        \"\"\"\n",
    "        Calculate the number of histogram bins using the Freedman-Diaconis rule.\n",
    "        \"\"\"\n",
    "        # Pre-process\n",
    "        data = np.asarray(data)\n",
    "        data = data[~np.isnan(data)]\n",
    "\n",
    "        q75, q25 = np.percentile(data, [75 ,25])\n",
    "        iqr = q75 - q25\n",
    "        n = len(data)\n",
    "\n",
    "        # Fallback: just one bin\n",
    "        if iqr == 0 or n <= 1:\n",
    "            return 1\n",
    "\n",
    "        bin_width = 2 * iqr / (n ** (1 / 3))\n",
    "\n",
    "        # Avoid division by zero\n",
    "        if bin_width == 0:\n",
    "            return 1\n",
    "\n",
    "        data_range = data.max() - data.min()\n",
    "        bins = int(np.ceil(data_range / bin_width))\n",
    "\n",
    "        # Always at least one bin\n",
    "        return max(bins, 1)\n",
    "\n",
    "    # Replace with your DataFrame and column name\n",
    "    df = df_melt.copy()\n",
    "    x_data = df[X_COL]\n",
    "    y_data = df[Y_COL]\n",
    "\n",
    "    # Calculate the number of bins using Freedman-Diaconis rule\n",
    "    x_bins = calculate_bins(x_data)\n",
    "    y_bins = calculate_bins(y_data)\n",
    "\n",
    "    # Bin the x and y data\n",
    "    df['x_binned'] = pd.cut(x_data, bins=x_bins)\n",
    "    df['y_binned'] = pd.cut(y_data, bins=y_bins)\n",
    "\n",
    "    # Create contingency table (cross-tab of binned values)\n",
    "    contingency_table = pd.crosstab(df['x_binned'], df['y_binned'])\n",
    "\n",
    "    # Perform chi-squared test\n",
    "    chi2, p, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "\n",
    "    # Write data to file\n",
    "    with open(analysis_str + f\"{chip}_\" + \"chi_square_results.txt\", \"w\") as f:\n",
    "        # Log results to file\n",
    "        f.write(f\"Chi-squared statistic: {chi2}\\n\")\n",
    "        f.write(f\"Degrees of freedom: {dof}\\n\")\n",
    "        f.write(f\"Optimal bin count: x = {x_bins}, y = {y_bins}\\n\")\n",
    "        f.write(f\"Expected frequencies:\\n{expected}\\n\")\n",
    "        f.write(f\"P-value: {p}\\n\")\n",
    "\n",
    "        # Interpret the result\n",
    "        if p < alpha:\n",
    "            f.write(\"Result: Reject the null hypothesis — association exists between binned x and y.\")\n",
    "        else:\n",
    "            f.write(\"Result: Fail to reject the null — no significant association.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc0a4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anova_test(y_vals: list, alpha: float = 0.05) -> None:\n",
    "    \"\"\"\n",
    "    Perform one-way ANOVA test\n",
    "    \"\"\"\n",
    "    # Number of groups and total number of observations\n",
    "    k = len(y_vals)\n",
    "    n = 0\n",
    "\n",
    "    for group in y_vals:\n",
    "        n += len(group)\n",
    "\n",
    "    # Perform one-way ANOVA\n",
    "    f_statistic, p_value = stats.f_oneway(*y_vals)\n",
    "\n",
    "    # Degrees of freedom\n",
    "    df_between = k - 1\n",
    "    df_within = n - k\n",
    "\n",
    "    # Calculate critical F value\n",
    "    f_critical = stats.f.ppf(1 - alpha, df_between, df_within)\n",
    "\n",
    "    # Write data to file\n",
    "    with open(analysis_str + \"anova_results.txt\", \"w\") as f:\n",
    "        # Log results to file\n",
    "        f.write(f\"F-statistic: {f_statistic:.5f}\\n\")\n",
    "        f.write(f\"p-value: {p_value:.5f}\\n\")\n",
    "        f.write(f\"F-critical (alpha = {alpha}): {f_critical:.5f}\\n\")\n",
    "\n",
    "        # Interpret the result\n",
    "        if f_statistic > f_critical:\n",
    "            f.write(f\"Reject the null hypothesis: At least one group mean is different.\")\n",
    "        else:\n",
    "            f.write(f\"Fail to reject the null hypothesis: No significant difference between group means.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d7a15a",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390072c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import (\n",
    "    ggplot,\n",
    "    aes,\n",
    "    geom_point,\n",
    "    geom_line,\n",
    "    ggtitle,\n",
    "    labs,\n",
    "    annotate,\n",
    "    xlim,\n",
    "    ylim,\n",
    "    scale_color_manual,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c8db00",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_LABEL = \"Pressure In (mb)\"\n",
    "Y_LABEL = \"Flow Rate Out (µL/min)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cca44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make variables for analysis\n",
    "data_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0285c3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph all the data\n",
    "for key in filtered_dict.keys():\n",
    "    df = pd.DataFrame()\n",
    "    chip_list = filtered_dict[key]\n",
    "\n",
    "    for i, df in enumerate(chip_list):\n",
    "        # Find applicable fits\n",
    "        log_eq      = find_log_fit(df)\n",
    "        lin_eq      = find_lin_fit(df)\n",
    "        logistic_eq = find_logistic_fit(df)\n",
    "\n",
    "        # Melt the data\n",
    "        df_melt = melt_data(df)\n",
    "\n",
    "        # Make the plot\n",
    "        plot =  (\n",
    "            ggplot(df_melt, aes(X_COL, Y_COL)) +\n",
    "            geom_point() +\n",
    "            geom_line(\n",
    "                aes(\n",
    "                    y='Fit Value',\n",
    "                    color='Fit Type'\n",
    "                )\n",
    "            ) +\n",
    "            scale_color_manual(\n",
    "                values = {\n",
    "                    'Logarithmic Fit': 'blue',\n",
    "                    'Linear Fit': 'red',\n",
    "                    'Logistic Fit': 'green'\n",
    "                }\n",
    "            ) +\n",
    "            ggtitle(f\"{Y_LABEL} vs {X_LABEL}\") +\n",
    "            labs(\n",
    "                x=X_LABEL,\n",
    "                y=Y_LABEL,\n",
    "                color=\"Model Fit\"\n",
    "            ) +\n",
    "            xlim(0, None) +\n",
    "            ylim(0, 400) +\n",
    "            annotate(\n",
    "                \"text\",\n",
    "                x=0,\n",
    "                y=400,\n",
    "                label=logistic_eq,\n",
    "                ha='left',\n",
    "                va='top',\n",
    "                size=8,\n",
    "                color='green'\n",
    "            ) +\n",
    "            annotate(\n",
    "                \"text\",\n",
    "                x=0,\n",
    "                y=350,\n",
    "                label=log_eq,\n",
    "                ha='left',\n",
    "                va='top',\n",
    "                size=8,\n",
    "                color='blue'\n",
    "            ) +\n",
    "            annotate(\n",
    "                \"text\",\n",
    "                x=0,\n",
    "                y=300,\n",
    "                label=lin_eq,\n",
    "                ha='left',\n",
    "                va='top',\n",
    "                size=8,\n",
    "                color='red'\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Save the plot\n",
    "        plot.save(graph_str + f\"{key}/graph_{i}.png\", width=10, height=6, dpi=300)\n",
    "\n",
    "        # Perform Chi-Square test\n",
    "        chi_square_test(df_melt, key)\n",
    "\n",
    "        # Add data to the analysis list\n",
    "        data_list.append(df_melt[Y_COL].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a352198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform data analysis\n",
    "anova_test(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e76248",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
